---
title: "Logistic Regression"
author: "Dhrubasattwata Roy Choudhury"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Logistic Regression 

If we use linear regression to model a dichotomous variable (as Y), the resulting model might not restrict the predicted Ys within 0 and 1. Besides, other assumptions of linear regression such as normality of errors may get violated.

### Dataset
Lets try and predict if an individual will earn more than $50K using logistic regression based on demographic variables available in the adult data. Download it from here: https://datahub.io/machine-learning/adult#resource-adult_zip

```{r}
inputData <- read.csv("C:/Users/Dhruba/Documents/R/data/adult.csv")
head(inputData) #first 6 rows
```

### Check Class bias
Ideally, the proportion of events and non-events in the Y variable should approximately be the same. So, lets first check the proportion of classes in the dependent variable ABOVE50K.

```{r}
table(inputData$ABOVE50K)
```

Clearly, there is a class bias, a condition observed when the proportion of events is much smaller than proportion of non-events. Sample the observations in approximately equal proportions to get better models.

### Create Training and Test Samples
One way to address the problem of class bias is to draw the 0’s and 1’s for the trainingData (development sample) in equal proportions.  The size of development sample will be smaller that validation, which is okay, because, there are large number of observations.

```{r}
# Create Training Data
input_ones <- inputData[which(inputData$ABOVE50K == 1), ]  # all 1's
input_zeros <- inputData[which(inputData$ABOVE50K == 0), ]  # all 0's
set.seed(100)  # for repeatability of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]  
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's 

# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's 
```

### Trying to understand the independent variables to understand their influence on Y
Compute Information Values
Using smbinning::smbinning function (converts a continuous variable into a categorical variable using recursive partitioning), convert them to categorical variables and then, capture the information values for all variables in iv_df.

```{r}
library(smbinning)
# segregate continuous and factor variables
factor_vars <- c ("WORKCLASS", "EDUCATION", "OCCUPATION", "RELATIONSHIP", "RACE", "SEX")
continuous_vars <- c("AGE", "FNLWGT","EDUCATION.NUM", "HOURSPERWEEK", "CAPITALGAIN", "CAPITALLOSS")

iv_df <- data.frame(VARS=c(factor_vars, continuous_vars), IV=numeric(12))  # init for IV results

# compute IV for categoricals
for(factor_var in factor_vars){
  smb <- smbinning.factor(trainingData, y="ABOVE50K", x=factor_var)  # WOE table
  if(class(smb) != "character"){ # heck if some error occured
    iv_df[iv_df$VARS == factor_var, "IV"] <- smb$iv
  }
}

# compute IV for continuous vars
for(continuous_var in continuous_vars){
  smb <- smbinning(trainingData, y="ABOVE50K", x=continuous_var)  # WOE table
  if(class(smb) != "character"){  # any error while calculating scores.
    iv_df[iv_df$VARS == continuous_var, "IV"] <- smb$iv
  }
}

iv_df <- iv_df[order(-iv_df$IV), ]  # sort
iv_df

```


### BUILD the Logistic Model and Predict

```{r}
logit.model <- glm(ABOVE50K ~ relationship + age + capitalgain + occupation + education.num, data=trainingData, family=binomial(link="logit"))
predicted <- predict(logit.model, testData, type="response") 

```

### Decide on optimal prediction probability cutoff for the model
The default cutoff prediction probability score is 0.5 or the ratio of 1’s and 0’s in the training data. But sometimes, tuning the probability cutoff can improve the accuracy in both the development and validation samples. The InformationValue::optimalCutoff function provides ways to find the optimal cutoff to improve the prediction of 1’s, 0’s, both 1’s and 0’s and o reduce the misclassification error. Lets compute the optimal score that minimizes the misclassification error for the above model.

```{r}
library(InformationValue)
optCutOff <- optimalCutoff(testData$ABOVE50K, predicted)[1] 
print (paste0("The optimized cutoff is ", optCutOff))
```

### Model Statistics

```{r}
summary(logit.model)
```

Misclassification error is the percentage mismatch of predcited vs actuals, irrespective of 1’s or 0’s. The lower the misclassification error, the better is your model.
```{r}
mis.error <- misClassError(testData$ABOVE50K, predicted, threshold = optCutOff)
print (paste0("The Misclassification error is ", mis.error))
```

### Performance Metrics

ROC

Receiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by a given logit model as the prediction probability cutoff is lowered from 1 to 0. It should as far away from the 45 degree line as possible.
```{r}
plotROC(testData$ABOVE50K, predicted)
```
The above model has area under ROC curve 89.42%, which is pretty good.


Concordance

Ideally, the model-calculated-probability-scores of all actual Positive’s, (aka Ones) should be greater than the model-calculated-probability-scores of ALL the Negatives (aka Zeroes). Such a model is said to be perfectly concordant and a highly reliable one. This phenomenon can be measured by Concordance and Discordance.
```{r}
Concordance(testData$ABOVE50K, predicted)
```
The above model with a concordance of 89.39% is indeed a good quality model.

Specificity and Sensitivity

Sensitivity (or True Positive Rate) is the percentage of 1’s (actuals) correctly predicted by the model, while, specificity is the percentage of 0’s (actuals) correctly predicted. Specificity can also be calculated as 1−False Positive Rate.

```{r}
sensitivity(testData$ABOVE50K, predicted, threshold = optCutOff)
specificity(testData$ABOVE50K, predicted, threshold = optCutOff)
```

Confusion Matrix
```{r}
confusionMatrix(testData$ABOVE50K, predicted, threshold = optCutOff)
```

The model was able to correctly identify 28376 + 1219 cases and misspecified 2288+599 points.




